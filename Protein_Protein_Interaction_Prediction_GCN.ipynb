{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:12.037320Z",
     "iopub.status.busy": "2021-04-01T09:18:12.036293Z",
     "iopub.status.idle": "2021-04-01T09:18:14.524162Z",
     "shell.execute_reply": "2021-04-01T09:18:14.524803Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "import biotite.structure as struc\n",
    "import biotite.structure.io as strucio\n",
    "import biotite.database.rcsb as rcsb\n",
    "from biotite.structure import filter_amino_acids\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sn\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.527516Z",
     "iopub.status.busy": "2021-04-01T09:18:14.526773Z",
     "iopub.status.idle": "2021-04-01T09:18:14.532066Z",
     "shell.execute_reply": "2021-04-01T09:18:14.532546Z"
    }
   },
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.test_size = 0.2\n",
    "args.shuffle = True\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.534832Z",
     "iopub.status.busy": "2021-04-01T09:18:14.534105Z",
     "iopub.status.idle": "2021-04-01T09:18:14.538229Z",
     "shell.execute_reply": "2021-04-01T09:18:14.538704Z"
    }
   },
   "outputs": [],
   "source": [
    "H1_dict = {'A' : 0.62, \n",
    "           'C' : 0.29, \n",
    "           'D' : -0.9, \n",
    "           'E' : -0.74, \n",
    "           'F' : 1.19, \n",
    "           'G' : 0.48, \n",
    "           'H' : -0.4, \n",
    "           'I' : 1.38, \n",
    "           'K' : -1.5, \n",
    "           'L' : 1.06, \n",
    "           'M' : 0.64, \n",
    "           'N' : -0.78, \n",
    "           'P' : 0.12, \n",
    "           'Q' : -0.85, \n",
    "           'R' : -2.53, \n",
    "           'S' : -0.18, \n",
    "           'T' : -0.05, \n",
    "           'V' : 1.08, \n",
    "           'W' : 0.81, \n",
    "           'Y' : 0.26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.540863Z",
     "iopub.status.busy": "2021-04-01T09:18:14.540145Z",
     "iopub.status.idle": "2021-04-01T09:18:14.544213Z",
     "shell.execute_reply": "2021-04-01T09:18:14.544681Z"
    }
   },
   "outputs": [],
   "source": [
    "H2_dict = {'A' : -0.5, \n",
    "           'C' : -1, \n",
    "           'D' : 3, \n",
    "           'E' : 3, \n",
    "           'F' : -2.5, \n",
    "           'G' : 0, \n",
    "           'H' : -0.5, \n",
    "           'I' : -1.8, \n",
    "           'K' : 3, \n",
    "           'L' : -1.8, \n",
    "           'M' : -1.3, \n",
    "           'N' : 2, \n",
    "           'P' : 0, \n",
    "           'Q' : 0.2, \n",
    "           'R' : 3, \n",
    "           'S' : 0.3, \n",
    "           'T' : -0.4, \n",
    "           'V' : -1.5, \n",
    "           'W' : -3.4, \n",
    "           'Y' : -2.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.546882Z",
     "iopub.status.busy": "2021-04-01T09:18:14.546164Z",
     "iopub.status.idle": "2021-04-01T09:18:14.550094Z",
     "shell.execute_reply": "2021-04-01T09:18:14.550560Z"
    }
   },
   "outputs": [],
   "source": [
    "PL_dict = {'A' : 8.1, \n",
    "           'C' : 5.5, \n",
    "           'D' : 13, \n",
    "           'E' : 12.3, \n",
    "           'F' : 5.2, \n",
    "           'G' : 9, \n",
    "           'H' : 10.4, \n",
    "           'I' : 5.2, \n",
    "           'K' : 11.3, \n",
    "           'L' : 4.9, \n",
    "           'M' : 5.7, \n",
    "           'N' : 11.6, \n",
    "           'P' : 8, \n",
    "           'Q' : 10.5, \n",
    "           'R' : 10.5, \n",
    "           'S' : 9.2, \n",
    "           'T' : 8.6, \n",
    "           'V' : 5.9, \n",
    "           'W' : 5.4, \n",
    "           'Y' : 6.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.552722Z",
     "iopub.status.busy": "2021-04-01T09:18:14.551999Z",
     "iopub.status.idle": "2021-04-01T09:18:14.555955Z",
     "shell.execute_reply": "2021-04-01T09:18:14.556424Z"
    }
   },
   "outputs": [],
   "source": [
    "SASA_dict = {'A' : 1.181, \n",
    "             'C' : 1.461, \n",
    "             'D' : 1.587, \n",
    "             'E' : 1.862, \n",
    "             'F' : 2.228, \n",
    "             'G' : 0.881, \n",
    "             'H' : 2.025, \n",
    "             'I' : 1.81, \n",
    "             'K' : 2.258, \n",
    "             'L' : 1.931, \n",
    "             'M' : 2.034, \n",
    "             'N' : 1.655, \n",
    "             'P' : 1.468, \n",
    "             'Q' : 1.932, \n",
    "             'R' : 2.56, \n",
    "             'S' : 1.298, \n",
    "             'T' : 1.525, \n",
    "             'V' : 1.645, \n",
    "             'W' : 2.663, \n",
    "             'Y' : 2.368}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.558622Z",
     "iopub.status.busy": "2021-04-01T09:18:14.557905Z",
     "iopub.status.idle": "2021-04-01T09:18:14.561977Z",
     "shell.execute_reply": "2021-04-01T09:18:14.562449Z"
    }
   },
   "outputs": [],
   "source": [
    "pKa_dict = {'A' : 2.34,\n",
    "            'R' : 2.17,\n",
    "            'N' : 2.02,\n",
    "            'D' : 1.88,\n",
    "            'C' : 1.96,\n",
    "            'E' : 2.19,\n",
    "            'Q' : 2.17,\n",
    "            'G' : 2.34,\n",
    "            'H' : 1.82,\n",
    "            'O' : 1.82,\n",
    "            'I' : 2.36,\n",
    "            'L' : 2.36,\n",
    "            'K' : 2.18,\n",
    "            'M' : 2.28,\n",
    "            'F' : 1.83,\n",
    "            'P' : 1.99,\n",
    "            'U' : 0,\n",
    "            'S' : 2.21,\n",
    "            'T' : 2.09,\n",
    "            'W' : 2.83,\n",
    "            'Y' : 2.20,\n",
    "            'V' : 2.32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.564589Z",
     "iopub.status.busy": "2021-04-01T09:18:14.563887Z",
     "iopub.status.idle": "2021-04-01T09:18:14.567927Z",
     "shell.execute_reply": "2021-04-01T09:18:14.568397Z"
    }
   },
   "outputs": [],
   "source": [
    "pKb_dict = {'A' : 9.69,\n",
    "            'R' : 9.04,\n",
    "            'N' : 8.80,\n",
    "            'D' : 9.60,\n",
    "            'C' : 10.28,\n",
    "            'E' : 9.67,\n",
    "            'Q' : 9.13,\n",
    "            'G' : 9.60,\n",
    "            'H' : 9.17,\n",
    "            'O' : 9.65,\n",
    "            'I' : 9.60,\n",
    "            'L' : 9.60,\n",
    "            'K' : 8.95,\n",
    "            'M' : 9.21,\n",
    "            'F' : 9.13,\n",
    "            'P' : 10.60,\n",
    "            'U' : 0,\n",
    "            'S' : 9.15,\n",
    "            'T' : 9.10,\n",
    "            'W' : 9.39,\n",
    "            'Y' : 9.11,\n",
    "            'V' : 9.62}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.570587Z",
     "iopub.status.busy": "2021-04-01T09:18:14.569871Z",
     "iopub.status.idle": "2021-04-01T09:18:14.573921Z",
     "shell.execute_reply": "2021-04-01T09:18:14.574388Z"
    }
   },
   "outputs": [],
   "source": [
    "pI_dict = {'A' : 6.00,\n",
    "           'R' : 10.76,\n",
    "           'N' : 5.41,\n",
    "           'D' : 2.77,\n",
    "           'C' : 5.07,\n",
    "           'E' : 3.22,\n",
    "           'Q' : 5.65,\n",
    "           'G' : 5.97,\n",
    "           'H' : 7.59,\n",
    "           'O' : 0,\n",
    "           'I' : 6.02,\n",
    "           'L' : 5.98,\n",
    "           'K' : 9.74,\n",
    "           'M' : 5.74,\n",
    "           'F' : 5.48,\n",
    "           'P' : 6.30,\n",
    "           'U' : 5.68,\n",
    "           'S' : 5.68,\n",
    "           'T' : 5.60,\n",
    "           'W' : 5.89,\n",
    "           'Y' : 5.66,\n",
    "           'V' : 5.96}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.576840Z",
     "iopub.status.busy": "2021-04-01T09:18:14.575967Z",
     "iopub.status.idle": "2021-04-01T09:18:14.601433Z",
     "shell.execute_reply": "2021-04-01T09:18:14.601907Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "AA = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "def aa_features(x):\n",
    "    return np.array(one_of_k_encoding(x, AA) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).aromaticity()), [0, 1]) +  \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).isoelectric_point()), [4, 5, 6, 8, 9]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).gravy()), [0, 1, 2, 3, 4, -4, -3, -1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[0]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[1]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[2]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molar_extinction_coefficient()[0]), [0, 1490, 5500]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molar_extinction_coefficient()[1]), [0, 1490, 5500]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molecular_weight()), [121, 131, 132, 165, 133, 105, 75, 204, 174, \n",
    "                                                                                   146, 115, 147, 149, 117, 119, 181, 89, 155]) + \n",
    "                    one_of_k_encoding(H1_dict[x], list(set(list(H1_dict.values())))) + \n",
    "                    one_of_k_encoding(H2_dict[x], list(set(list(H2_dict.values())))) + \n",
    "                    one_of_k_encoding(PL_dict[x], list(set(list(PL_dict.values())))) + \n",
    "                    one_of_k_encoding(SASA_dict[x], list(set(list(SASA_dict.values())))) + \n",
    "                    one_of_k_encoding(pKa_dict[x], list(set(list(pKa_dict.values())))) + \n",
    "                    one_of_k_encoding(pKb_dict[x], list(set(list(pKb_dict.values())))) + \n",
    "                    one_of_k_encoding(pI_dict[x], list(set(list(pI_dict.values())))))\n",
    "\n",
    "def adjacency2edgeindex(adjacency):\n",
    "    start = []\n",
    "    end = []\n",
    "    adjacency = adjacency - np.eye(adjacency.shape[0], dtype=int)\n",
    "    for x in range(adjacency.shape[1]):\n",
    "        for y in range(adjacency.shape[0]):\n",
    "            if adjacency[x, y] == 1:\n",
    "                start.append(x)\n",
    "                end.append(y)\n",
    "\n",
    "    edge_index = np.asarray([start, end])\n",
    "    return edge_index\n",
    "\n",
    "AMINOS =  ['CYS', 'ASP', 'SER', 'GLN', 'LYS', 'ILE', 'PRO', 'THR', 'PHE', 'ASN', \n",
    "           'GLY', 'HIS', 'LEU', 'ARG', 'TRP', 'ALA', 'VAL', 'GLU', 'TYR', 'MET']\n",
    "def filter_20_amino_acids(array):\n",
    "    return ( np.in1d(array.res_name, AMINOS) & (array.res_id != -1) )\n",
    "\n",
    "# RNA Graph (1Q8N) \n",
    "# filter_20_amino_acids -> filter_seq\n",
    "SEQ = ['A', 'T', 'G', 'C', 'U']\n",
    "def filter_seq(array):\n",
    "    return ( np.in1d(array.res_name, SEQ) & (array.res_id != -1) )\n",
    "\n",
    "def protein_analysis(pdb_id):\n",
    "    file_name = rcsb.fetch(pdb_id, \"mmtf\", './data/pdb')\n",
    "    array = strucio.load_structure(file_name)\n",
    "#     protein_mask = filter_amino_acids(array)\n",
    "    protein_mask = filter_20_amino_acids(array)\n",
    "    try:\n",
    "        array = array[protein_mask]\n",
    "    except:\n",
    "        array = array[0]\n",
    "        array = array[protein_mask]\n",
    "    try:\n",
    "        ca = array[array.atom_name == \"CA\"]\n",
    "    except:\n",
    "        array = array[0]\n",
    "        ca = array[array.atom_name == \"CA\"]\n",
    "    \n",
    "    seq = ''.join([three_to_one(str(i).split(' CA')[0][-3:]) for i in ca])\n",
    "    # 7 Angstrom adjacency threshold\n",
    "    threshold = 7\n",
    "    # Create cell list of the CA atom array\n",
    "    # for efficient measurement of adjacency\n",
    "    cell_list = struc.CellList(ca, cell_size=threshold)\n",
    "    A = cell_list.create_adjacency_matrix(threshold)\n",
    "    A = np.where(A == True, 1, A)\n",
    "\n",
    "    return [aa_features(aa) for aa in seq], adjacency2edgeindex(A)\n",
    "\n",
    "def pro2vec(pdb_id):\n",
    "    node_f, edge_index = protein_analysis(pdb_id)\n",
    "    data = Data(x=torch.tensor(node_f, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long))\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "def make_pro(df):\n",
    "    pro1_key = []\n",
    "    pro2_key = []\n",
    "    pro_value = []\n",
    "    for i in range(df.shape[0]):\n",
    "        pro1_key.append(df['Protein_1'].iloc[i])\n",
    "        pro2_key.append(df['Protein_2'].iloc[i])\n",
    "        pro_value.append(df['PPI'].iloc[i])\n",
    "    return pro1_key, pro2_key, pro_value\n",
    "\n",
    "def save_graph(graph_path, pdb_id):\n",
    "    vec = pro2vec(pdb_id)\n",
    "    np.save(graph_path+pdb_id+'_e.npy', vec.edge_index)\n",
    "    np.save(graph_path+pdb_id+'_n.npy', vec.x)\n",
    "    \n",
    "def load_graph(graph_path, pdb_id):\n",
    "    n = np.load(graph_path+pdb_id+'_n.npy')\n",
    "    e = np.load(graph_path+pdb_id+'_e.npy')\n",
    "    N = torch.tensor(n, dtype=torch.float)\n",
    "    E = torch.tensor(e, dtype=torch.long)\n",
    "    data = Data(x=N, edge_index=E)\n",
    "    return data\n",
    "\n",
    "def make_vec(pro1, pro2, value):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y = []\n",
    "    for i in range(len(pro1)):\n",
    "        m1 = pro1[i]\n",
    "        m2 = pro2[i]\n",
    "        y = value[i]\n",
    "        try:\n",
    "            v1 = load_graph('./data/graph/', m1)\n",
    "            v2 = load_graph('./data/graph/', m2)\n",
    "            if v1.x.shape[0] < 100000:\n",
    "                if v2.x.shape[0] < 100000:\n",
    "                    X1.append(v1)\n",
    "                    X2.append(v2)\n",
    "                    Y.append(y)\n",
    "        except:\n",
    "            continue\n",
    "    for i, data in enumerate(X1):\n",
    "        y = Y[i]\n",
    "        data.y = torch.tensor([y], dtype=torch.long)\n",
    "    for i, data in enumerate(X2):\n",
    "        y = Y[i]\n",
    "        data.y = torch.tensor([y], dtype=torch.long)\n",
    "    return X1, X2\n",
    "\n",
    "def df_check(df):\n",
    "    df['pro2vec'] = 'Yes'\n",
    "    for i in range(df.shape[0]):\n",
    "        try:\n",
    "            save_graph('./data/graph/', df['Protein_1'].iloc[i])\n",
    "            save_graph('./data/graph/', df['Protein_2'].iloc[i])\n",
    "        except:\n",
    "            df['pro2vec'].iloc[i] = 'No'\n",
    "            continue\n",
    "    df = df[df['pro2vec'] != 'No'].reset_index(drop=True)\n",
    "    del df['pro2vec']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "graph = pro2vec('3ETU')\n",
    "G = to_networkx(graph)\n",
    "plt.figure(1,figsize=(10, 10)) \n",
    "nx.draw(G, node_size=50,linewidths=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "https://github.com/baranwa2/Struct2Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./interactions_data.txt', sep='\\t', header=None)\n",
    "# df.columns = ['Protein_1', 'Protein_2', 'PPI']\n",
    "# !mkdir data\n",
    "# !mkdir data/pdb\n",
    "# !mkdir data/graph\n",
    "# df = df_check(df)\n",
    "# df.to_csv('./PGCN_PPI_DB.csv', index=False)\n",
    "df = pd.read_csv('./PGCN_PPI_DB.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.978520Z",
     "iopub.status.busy": "2021-04-01T09:18:14.977796Z",
     "iopub.status.idle": "2021-04-01T09:18:14.984216Z",
     "shell.execute_reply": "2021-04-01T09:18:14.984690Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021, stratify=df['PPI'])\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pro1_key, train_pro2_key, train_pro_value = make_pro(X_train)\n",
    "test_pro1_key, test_pro2_key, test_pro_value = make_pro(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X1, train_X2 = make_vec(train_pro1_key, train_pro2_key, train_pro_value)\n",
    "test_X1, test_X2 = make_vec(test_pro1_key, test_pro2_key, test_pro_value)\n",
    "len(train_X1), len(train_X2), len(test_X1), len(test_X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "for i in range(len(train_X1)):\n",
    "    train_X.append([train_X1[i], train_X2[i]])\n",
    "test_X = []\n",
    "for i in range(len(test_X1)):\n",
    "    test_X.append([test_X1[i], test_X2[i]])\n",
    "\n",
    "print('- Train Data :', len(train_X))\n",
    "print('- Test Data :', len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:53:05.412072Z",
     "iopub.status.busy": "2021-04-01T09:53:05.411269Z",
     "iopub.status.idle": "2021-04-01T09:53:05.425685Z",
     "shell.execute_reply": "2021-04-01T09:53:05.426170Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNlayer(nn.Module):\n",
    "    def __init__(self, n_features, conv_dim1, conv_dim2, conv_dim3, concat_dim, dropout):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.conv_dim1 = conv_dim1\n",
    "        self.conv_dim2 = conv_dim2\n",
    "        self.conv_dim3 = conv_dim3\n",
    "        self.concat_dim =  concat_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GCNConv(self.n_features, self.conv_dim1, cached=False)\n",
    "        self.bn1 = BatchNorm1d(self.conv_dim1)\n",
    "        self.conv2 = GCNConv(self.conv_dim1, self.conv_dim2, cached=False)\n",
    "        self.bn2 = BatchNorm1d(self.conv_dim2)\n",
    "        self.conv3 = GCNConv(self.conv_dim2, self.conv_dim3, cached=False)\n",
    "        self.bn3 = BatchNorm1d(self.conv_dim3)\n",
    "        self.conv4 = GCNConv(self.conv_dim3, self.concat_dim, cached=False)\n",
    "        self.bn4 = BatchNorm1d(self.concat_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "    \n",
    "class FClayer(nn.Module):\n",
    "    def __init__(self, concat_dim, pred_dim1, pred_dim2, pred_dim3, out_dim, dropout):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.concat_dim = concat_dim\n",
    "        self.pred_dim1 = pred_dim1\n",
    "        self.pred_dim2 = pred_dim2\n",
    "        self.pred_dim3 = pred_dim3\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc1 = Linear(self.concat_dim*2, self.pred_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.pred_dim1)\n",
    "        self.fc2 = Linear(self.pred_dim1, self.pred_dim2)\n",
    "        self.bn2 = BatchNorm1d(self.pred_dim2)\n",
    "        self.fc3 = Linear(self.pred_dim2, self.pred_dim3)\n",
    "        self.fc4 = Linear(self.pred_dim3, self.out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.fc1(data))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNlayer(args.n_features, \n",
    "                              args.conv_dim1, \n",
    "                              args.conv_dim2, \n",
    "                              args.conv_dim3, \n",
    "                              args.concat_dim, \n",
    "                              args.dropout)\n",
    "        \n",
    "        self.conv2 = GCNlayer(args.n_features, \n",
    "                              args.conv_dim1, \n",
    "                              args.conv_dim2, \n",
    "                              args.conv_dim3, \n",
    "                              args.concat_dim, \n",
    "                              args.dropout)\n",
    "\n",
    "        self.fc = FClayer(args.concat_dim, \n",
    "                          args.pred_dim1, \n",
    "                          args.pred_dim2, \n",
    "                          args.pred_dim3, \n",
    "                          args.out_dim, \n",
    "                          args.dropout)\n",
    "        \n",
    "    def forward(self, pro1, pro2):\n",
    "        x1 = self.conv1(pro1)\n",
    "        x2 = self.conv2(pro2)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:53:05.428555Z",
     "iopub.status.busy": "2021-04-01T09:53:05.427794Z",
     "iopub.status.idle": "2021-04-01T09:53:05.440098Z",
     "shell.execute_reply": "2021-04-01T09:53:05.440575Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename):\n",
    "    state = {\n",
    "        'Epoch': epoch,\n",
    "        'State_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def train(model, device, optimizer, train_loader, criterion, args):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    epoch_train_loss = 0\n",
    "    for i, [pro1, pro2] in enumerate(train_loader):\n",
    "        pro1 = pro1.to(device)\n",
    "        pro2 = pro2.to(device)\n",
    "        labels = pro1.y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pro1, pro2)\n",
    "        outputs.require_grad = False\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_acc =  100 * train_correct / train_total\n",
    "    print('- Loss : %.4f' % epoch_train_loss)\n",
    "    print('- Accuracy : %.4f' % train_acc)\n",
    "    return model, epoch_train_loss, train_acc\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, args):\n",
    "    model.eval()\n",
    "    classes = ('0', '1')\n",
    "    class_correct = list(0. for i in range(len(classes)))\n",
    "    class_total = list(0. for i in range(len(classes)))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    nb_classes = len(classes)\n",
    "    confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    test_hist = {\"test_acc\":[]}\n",
    "    y_score =[]\n",
    "    y_test =[]\n",
    "    data_total = []\n",
    "    pred_data_total = []\n",
    "    roc_total = dict()\n",
    "    with torch.no_grad():\n",
    "        for i, [pro1, pro2] in enumerate(test_loader):\n",
    "            pro1 = pro1.to(device)\n",
    "            pro2 = pro2.to(device)\n",
    "            labels = pro1.y.to(device)\n",
    "            data_total += pro1.y.tolist()\n",
    "            outputs = model(pro1, pro2)\n",
    "            pred_data_total += outputs.view(-1).tolist()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            y_score.append(outputs.cpu().numpy())\n",
    "            y_test.append(labels.cpu().numpy())\n",
    "            for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "            for i in range(labels.shape[0]):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                test_hist[\"test_acc\"].append((predicted == labels).sum().item())\n",
    "                \n",
    "    # ROC Curve\n",
    "    y_test_list = []\n",
    "    for i in y_test[0]:\n",
    "        t_list = [0, 0, 0]\n",
    "        t_list[i] = 1\n",
    "        y_test_list.append(t_list)\n",
    "    roc = dict()\n",
    "    roc['name'] = str(args.exp_name)\n",
    "    roc['y_score'] = y_score[0].tolist()\n",
    "    roc['y_test'] = y_test_list\n",
    "    roc_total[str(args.exp_name)] = roc\n",
    "    # save data\n",
    "    with open('PPI.pickle','wb') as fw:\n",
    "        pickle.dump(roc_total, fw)\n",
    "\n",
    "#     data_total = list(labels.view(-1).cpu().numpy())\n",
    "#     pred_data_total = list(predicted.view(-1).cpu().numpy())\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_score][0]\n",
    "    conf = confusion_matrix.tolist()\n",
    "    total_acc = 100 * correct / total\n",
    "    low = 100 * class_correct[0] / class_total[0]\n",
    "    medium = 100 * class_correct[1] / class_total[1]\n",
    "    miscore = f1_score(data_total, pred_data_total, average='micro')\n",
    "    mascore = f1_score(data_total, pred_data_total, average='macro')\n",
    "    print()\n",
    "    print('[Test]')\n",
    "    print('- Total Accuracy : %d %%' % (100 * correct / total))            \n",
    "    print('- Accuracy of 0 : %2d %%' % (low))\n",
    "    print('- Accuracy of 1 : %2d %%' % (medium))\n",
    "    print('- F-1 Micro Score : %.2f' % (float(miscore)))\n",
    "    print('- F-1 Macro Score : %.2f' % (float(mascore)))\n",
    "    return conf, total_acc, low, medium, miscore, mascore, data_total, pred_data_total, y_pred_list\n",
    "\n",
    "\n",
    "def experiment(model, train_loader, test_loader, device, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    \n",
    "    list_train_loss = list()\n",
    "    list_train_acc = list()\n",
    "    print('[Train]')\n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        print('- Epoch :', epoch+1)\n",
    "        model, train_loss, train_acc = train(model, device, optimizer, train_loader, criterion, args)\n",
    "        list_train_loss.append(train_loss)\n",
    "        list_train_acc.append(train_acc)\n",
    "    \n",
    "    conf, total_acc, low, medium, miscore, mascore, data_total, pred_data_total, y_pred_list = test(model, device, test_loader, args)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_train_acc = list_train_acc\n",
    "    args.data_total = data_total\n",
    "    args.pred_data_total = pred_data_total\n",
    "    args.conf = conf\n",
    "    args.total_acc = total_acc\n",
    "    args.low = low\n",
    "    args.medium = medium\n",
    "    args.miscore = miscore\n",
    "    args.mascore = mascore\n",
    "    args.time_required = time_required\n",
    "    args.y_pred_list = y_pred_list\n",
    "    \n",
    "    save_checkpoint(epoch, model, optimizer, './mymodel.pt')\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:53:05.442784Z",
     "iopub.status.busy": "2021-04-01T09:53:05.442061Z",
     "iopub.status.idle": "2021-04-01T13:35:55.366307Z",
     "shell.execute_reply": "2021-04-01T13:35:55.367062Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args.batch_size = 32\n",
    "args.epoch = 100\n",
    "args.lr = 0.00001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 10\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.1\n",
    "args.n_features = 194\n",
    "args.conv_dim1 = 128\n",
    "args.conv_dim2 = 128\n",
    "args.conv_dim3 = 128\n",
    "args.concat_dim = 128\n",
    "args.pred_dim1 = 128\n",
    "args.pred_dim2 = 128\n",
    "args.pred_dim3 = 128\n",
    "args.out_dim = 2\n",
    "\n",
    "model = Net(args)\n",
    "model = model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_X, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_X, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "dict_result = dict()\n",
    "args.exp_name = 'Test'\n",
    "result = vars(experiment(model, train_loader, test_loader, device, args))\n",
    "dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "result_df = pd.DataFrame(dict_result).transpose()\n",
    "result_df.to_json('PPI.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1')\n",
    "df = pd.read_json('PPI.JSON', orient='table')\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "plt.suptitle(args.exp_name, fontsize=16)\n",
    "\n",
    "train_loss = df['list_train_loss'].iloc[0]\n",
    "train_acc = df['list_train_acc'].iloc[0]\n",
    "c = df['conf'].iloc[0]\n",
    "confusion_matrix = np.array([c[0], c[1]], dtype=float)\n",
    "df_cm = pd.DataFrame(confusion_matrix, range(len(classes)), range(len(classes)))\n",
    "accuracy = df['total_acc'].iloc[0]\n",
    "accuracy_1 = df['low'].iloc[0]\n",
    "accuracy_2 = df['medium'].iloc[0]\n",
    "acc_list = [accuracy, accuracy_1, accuracy_2]\n",
    "y_true = df['data_total'].iloc[0]\n",
    "y_pred = df['pred_data_total'].iloc[0]\n",
    "miscore = df['miscore'].iloc[0]\n",
    "mascore = df['mascore'].iloc[0]\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([e for e in range(len(train_loss))], [float(t) for t in train_loss], label=\"train_loss\", c='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.ylim([0, 1])\n",
    "plt.plot([e for e in range(len(train_acc))], [float(t)*0.01 for t in train_acc], label=\"train_acc\", c='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(2, 2, 3)\n",
    "sn.heatmap(df_cm.astype('int'), annot=True, cmap='Blues', fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel(\"Class\")\n",
    "barlist = plt.barh(['Total', 'Low', 'Medium'], acc_list, height=0.4, color='cornflowerblue')\n",
    "barlist[0].set_color('mediumblue')\n",
    "for i, v in enumerate(acc_list):\n",
    "    plt.text(v-12, i-0.04, str(round(v,2)), color='white', fontweight='bold')\n",
    "acc1_test = 'Accuracy : ' + str(round(float(accuracy), 2))\n",
    "acc2_test = 'Low : ' + str(round(float(accuracy_1), 2))\n",
    "acc3_test = 'Medium : ' + str(round(float(accuracy_2), 2))\n",
    "mi_test = 'F-1 Micro : ' + str(round(float(miscore), 2))\n",
    "ma_test = 'F-1 Macro : ' + str(round(float(mascore), 2))\n",
    "plt.text(0, -0.85, acc1_test, fontsize=12)\n",
    "plt.text(0, -1.00, acc2_test, fontsize=12)\n",
    "plt.text(0, -1.15, acc3_test, fontsize=12)\n",
    "plt.text(0, -1.45, mi_test, fontsize=12)\n",
    "plt.text(0, -1.6, ma_test, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('PPI.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.clf()\n",
    "with open('PPI.pickle', 'rb') as fr:\n",
    "    result_loaded = pickle.load(fr)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "for i in result_loaded:\n",
    "    y_test = np.array(result_loaded[i]['y_test'])\n",
    "    y_score = np.array(result_loaded[i]['y_score'])\n",
    "    lw = 2\n",
    "    n_classes = len(classes)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "    plt.suptitle(args.exp_name, fontsize=16)\n",
    "    axes[0].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "    axes[0].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "    colors = (['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        axes[0].plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "    axes[0].plot([0, 1], [0, 1], c='black')\n",
    "    axes[0].axis(xmin=0, xmax=1, ymin=0, ymax=1.05)\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curve')\n",
    "    axes[0].legend(loc=\"lower right\")\n",
    "    axes[1].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "    axes[1].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "    axes[1].plot([0, 1], [0, 1], c='black')\n",
    "    axes[1].axis(xmin=0, xmax=1, ymin=0, ymax=1.05)\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('ROC Curve')\n",
    "    axes[1].legend(loc=\"lower right\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('PPI_ROC.png')\n",
    "    plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
